{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import time\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "print(\"Is CUDA available:\", is_cuda_available)\n",
    "\n",
    "# Determine the device to use: GPU (CUDA), Apple Silicon (MPS), or CPU\n",
    "DEVICE = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test tensor on CUDA: tensor([1., 2., 3.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Test tensor operation on GPU\n",
    "    test_tensor = torch.tensor([1.0, 2.0, 3.0], device=\"cuda\")\n",
    "    print(\"Test tensor on CUDA:\", test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully moved a tensor to the device: tensor([1, 2, 3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if is_cuda_available:\n",
    "    try:\n",
    "        test_tensor = torch.tensor([1, 2, 3], device=DEVICE)\n",
    "        print(\"Successfully moved a tensor to the device:\", test_tensor)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error moving a tensor to the device:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining VAE classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=20, w_dim=10):\n",
    "        super(StandardVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder layers\n",
    "        # Input: [bs, 1, 28, 28]\n",
    "        self.enc_conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)  # Output: [bs, 16, 14, 14]\n",
    "        self.enc_conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1) # Output: [bs, 32, 7, 7]\n",
    "        self.enc_fc1 = nn.Linear(32 * 7 * 7, w_dim)  # Output: [bs, 128]\n",
    "        # Two output layers for the latent space\n",
    "        self.enc_fc2 = nn.Linear(w_dim, latent_dim)  # For mu, Output: [bs, latent_dim]\n",
    "        self.enc_fc3 = nn.Linear(w_dim, latent_dim)  # For logvar, Output: [bs, latent_dim]\n",
    "\n",
    "        # Decoder layers\n",
    "        self.dec_fc1 = nn.Linear(latent_dim, w_dim)  # Output: [bs, 128]\n",
    "        self.dec_fc2 = nn.Linear(w_dim, 32 * 7 * 7)  # Output: [bs, 1568]\n",
    "        self.dec_conv1 = nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1)  # Output: [bs, 16, 14, 14]\n",
    "        self.dec_conv2 = nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1)  # Output: [bs, 1, 28, 28]\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.enc_conv1(x))\n",
    "        h = F.relu(self.enc_conv2(h))\n",
    "        h = torch.flatten(h, start_dim=1)\n",
    "        h = F.relu(self.enc_fc1(h))\n",
    "        return self.enc_fc2(h), self.enc_fc3(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        # Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.dec_fc1(z)) \n",
    "        h = F.relu(self.dec_fc2(h)).view(-1, 32, 7, 7) # .view reshapes [bs, 1568] to [bs, 32, 7, 7]\n",
    "        h = F.relu(self.dec_conv1(h))\n",
    "        return torch.sigmoid(self.dec_conv2(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), z, mu, logvar\n",
    "\n",
    "def loss_function_standard(recon_x, z, mu, logvar, x):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    # Can reweight BCE + KLD as desired\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LakeVAE inheirits from StandardVAE, but with a modified forward pass.\n",
    "Its encoding and decoding layers are identicle. \n",
    "The only difference is that the forward pass stores and returns all the intermediate values required to calculate the modified reconstruction loss.\n",
    "\n",
    "Since pixel values are either 0 or 1, we can use BCE between the input image and output image.\n",
    "However, for the intermediate layers, that are continuous, we use can MSE instead. This is also what the paper's code does in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LakeVAE(StandardVAE):\n",
    "    def forward(self, x):\n",
    "        # Encoder layers\n",
    "        # Input: [bs, 1, 28, 28]\n",
    "        enc_conv1_out = F.relu(self.enc_conv1(x))  # Output: [bs, 16, 14, 14]\n",
    "        enc_conv2_out = F.relu(self.enc_conv2(enc_conv1_out))  # Output: [bs, 32, 7, 7]\n",
    "        flattened = torch.flatten(enc_conv2_out, start_dim=1)  # Output: [bs, 1568]\n",
    "        \n",
    "        # w = F.relu(self.enc_fc1(flattened))  # Output: [bs, 128] # Eqn (5) in LAKE paper\n",
    "        w = self.enc_fc1(flattened)  # Not sure if relu is used in the paper or not. Output: [bs, 128] # Eqn (5) in LAKE paper\n",
    "        \n",
    "        mu, logvar = self.enc_fc2(w), self.enc_fc3(w)  # Output: [bs, latent_dim], [bs, latent_dim] # Eqn (6) in LAKE paper\n",
    "\n",
    "        # Reparameterization and Decoding layers\n",
    "        z = self.reparameterize(mu, logvar)  # Output: [bs, latent_dim] # Eqn (7) in LAKE paper\n",
    "        dec_fc1_out = F.relu(self.dec_fc1(z))  # Output: [bs, 128]\n",
    "        dec_fc2_out = F.relu(self.dec_fc2(dec_fc1_out)).view(-1, 32, 7, 7)  # Output: [bs, 1568], then reshaped to [bs, 32, 7, 7]\n",
    "        dec_conv1_out = F.relu(self.dec_conv1(dec_fc2_out))  # Output: [bs, 16, 14, 14]\n",
    "        recon_x = torch.sigmoid(self.dec_conv2(dec_conv1_out))  # Output: [bs, 1, 28, 28]\n",
    "\n",
    "        return recon_x, z, mu, logvar, enc_conv1_out, enc_conv2_out, w, dec_fc1_out, dec_fc2_out, dec_conv1_out\n",
    "\n",
    "def loss_function_lake(recon_x, z, mu, logvar, enc_conv1_out, enc_conv2_out, w, dec_fc1_out, dec_fc2_out, dec_conv1_out, x):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    # Layer-wise reconstruction loss\n",
    "    layer_loss = F.mse_loss(enc_conv1_out, dec_conv1_out) + F.mse_loss(enc_conv2_out, dec_fc2_out) + F.mse_loss(w, dec_fc1_out)\n",
    "    # KL Divergence\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD + layer_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definining Utility Classes for training and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, path, device):\n",
    "    model.load_state_dict(torch.load(path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, latent_vectors, folder=\"generated_images\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.decode(latent_vectors).cpu()\n",
    "    for i, img in enumerate(generated):\n",
    "        plt.imshow(img.squeeze(), cmap='gray')\n",
    "        plt.savefig(f\"{folder}/img_{epoch}_{i}.png\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space(model, data_loader, device, num_samples=1000):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, label in data_loader:\n",
    "            data = data.to(device)\n",
    "            mu, _ = model.encode(data)\n",
    "            latents.append(mu.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "            if len(latents) * data_loader.batch_size > num_samples:\n",
    "                break\n",
    "    \n",
    "    latents = np.concatenate(latents, axis=0)[:num_samples]\n",
    "    labels = np.concatenate(labels, axis=0)[:num_samples]\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    latents_reduced = pca.fit_transform(latents)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(latents_reduced[:, 0], latents_reduced[:, 1], c=labels, cmap='viridis', s=2, alpha=0.6)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Latent Space (PCA-reduced)\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.show()\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_processing = False\n",
    "def plot_losses_interactive(model, model_name, model_states, train_losses, val_losses, train_loader, val_loader, device, best_train_loss, best_val_loss):\n",
    "    global is_processing\n",
    "    epochs = len(train_losses)\n",
    "    fig = go.FigureWidget()\n",
    "\n",
    "    # Add traces for training and validation losses\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, epochs + 1)), y=train_losses, mode='lines+markers', name='Training Loss'))\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, epochs + 1)), y=val_losses, mode='lines+markers', name='Validation Loss'))\n",
    "\n",
    "    # Set layout for the plot\n",
    "    fig.update_layout(\n",
    "        title=f'Interactive {model_name} Training and Validation Loss',\n",
    "        xaxis_title='Epoch',\n",
    "        yaxis_title='Loss',\n",
    "        width=800, height=600\n",
    "    )\n",
    "\n",
    "    # Function to update the image on clicking a point on the plot\n",
    "    def update_image(trace, points, selector):\n",
    "        global is_processing\n",
    "        if is_processing:\n",
    "            return\n",
    "        is_processing = True\n",
    "    \n",
    "        if points.point_inds:\n",
    "            epoch = points.point_inds[0]\n",
    "            model.load_state_dict(model_states[epoch])\n",
    "            model.eval()\n",
    "    \n",
    "            # Determine which dataloader and best loss to use based on which trace was clicked\n",
    "            if trace.name == 'Training Loss':\n",
    "                data_loader = train_loader\n",
    "                current_loss = train_losses[epoch]\n",
    "                best_loss = best_train_loss\n",
    "                loss_type = 'Training'\n",
    "            elif trace.name == 'Validation Loss':\n",
    "                data_loader = val_loader\n",
    "                current_loss = val_losses[epoch]\n",
    "                best_loss = best_val_loss\n",
    "                loss_type = 'Validation'\n",
    "\n",
    "            # Calculate loss as a percentage of the best loss\n",
    "            loss_percentage = (best_loss / current_loss) * 100\n",
    "\n",
    "            # Generate and display an image\n",
    "            data, _ = next(iter(data_loader))\n",
    "            data = data.to(device)\n",
    "            reconstructed_img = model(data)[0].cpu().squeeze()\n",
    "\n",
    "            if reconstructed_img.ndim == 3:  # If image has 3 dimensions, take the first one\n",
    "                reconstructed_img = reconstructed_img[0]\n",
    "\n",
    "            # Display information and the image\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            plt.imshow(reconstructed_img.detach().numpy(), cmap='gray')\n",
    "            plt.title(f'{model_name} {loss_type} Loss\\nEpoch: {epoch + 1}\\nLoss: {current_loss:.4f} ({loss_percentage:.2f}% rel to best)')\n",
    "            plt.axis('off')\n",
    "            clear_output(wait=True)\n",
    "            display(plt.gcf())\n",
    "            model.train()\n",
    "    \n",
    "        is_processing = False\n",
    "\n",
    "    # Attach the click handler to the plot\n",
    "    fig.data[0].on_click(update_image)  # For training loss\n",
    "    fig.data[1].on_click(update_image)  # For validation loss\n",
    "\n",
    "    # Display the plot\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_vae(model, val_loader, loss_function, device):\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in val_loader:\n",
    "            data = data.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = loss_function(*outputs, data)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modify train_vae to instead to the probability density estimation training.\n",
    "I'm not sure how its training, or what this is doing, considering there's no updates to anything?\n",
    "write the functions for rec_euclidean and rec_cosine.\n",
    "The first thing the VAE classes return with their forward pass is their reconstructed images x'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, train_loader, val_loader, loss_function, optimiser, epochs, device, model_name, plot_interval=1):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    model_states = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = loss_function(*outputs, data)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss = total_loss / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = validate_vae(model, val_loader, loss_function, device)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Save the current model state\n",
    "        model_states.append(model.state_dict().copy())\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            best_epoch = epoch\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "        print(f'Saving {model_name} model at epoch {epoch+1}')\n",
    "        save_model(model, f'{model_name}_{epoch+1}.pth')\n",
    "        \n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model_path = f\"best_{model_name}_epoch_{best_epoch}.pth\"\n",
    "        save_model(model, model_path)\n",
    "        print(f\"Best {model_name} model saved as {model_path}\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    \n",
    "    best_train_loss = min(train_losses)\n",
    "    best_val_loss = min(val_losses)\n",
    "    plot_losses_interactive(model, model_name, model_states, train_losses, val_losses, train_loader, val_loader, device, best_train_loss, best_val_loss)\n",
    "        \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell loads in the mnist dataset into train, validation, and test dataloaders.\n",
    "While doing so, they are normalised to be [0,1] and turned into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download the MNIST dataset\n",
    "mnist_trainset = datasets.MNIST(root='~/.pytorch/MNIST_data/', train=True, download=True, transform=transform)\n",
    "\n",
    "# Splitting the dataset into train and validation sets\n",
    "train_size = int(0.8 * len(mnist_trainset))\n",
    "validation_size = len(mnist_trainset) - train_size\n",
    "train_dataset, validation_dataset = random_split(mnist_trainset, [train_size, validation_size])\n",
    "\n",
    "# Download and load the test data\n",
    "test_dataset = datasets.MNIST(root='~/.pytorch/MNIST_data/', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "validationloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "Standard_VAE = StandardVAE().to(DEVICE)\n",
    "Lake_VAE = LakeVAE().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameters: batch_size=64, learning_rate=0.001, epochs=10\n",
      "Training Standard_VAE...\n",
      "Epoch 1, Training Loss: 226.89376039632162, Validation Loss: 193.1483650716146\n",
      "Saving Standard_VAE model at epoch 1\n",
      "Epoch 2, Training Loss: 184.66886783854167, Validation Loss: 174.04674861653646\n",
      "Saving Standard_VAE model at epoch 2\n",
      "Epoch 3, Training Loss: 164.75998073323566, Validation Loss: 159.01668273925782\n",
      "Saving Standard_VAE model at epoch 3\n",
      "Epoch 4, Training Loss: 156.47857543945312, Validation Loss: 154.85309574381512\n",
      "Saving Standard_VAE model at epoch 4\n",
      "Epoch 5, Training Loss: 153.54923905436198, Validation Loss: 152.9229608561198\n",
      "Saving Standard_VAE model at epoch 5\n",
      "Epoch 6, Training Loss: 151.72211771647136, Validation Loss: 151.54623213704426\n",
      "Saving Standard_VAE model at epoch 6\n",
      "Epoch 7, Training Loss: 150.495251180013, Validation Loss: 150.29708162434895\n",
      "Saving Standard_VAE model at epoch 7\n",
      "Epoch 8, Training Loss: 149.39842336018881, Validation Loss: 149.78097778320313\n",
      "Saving Standard_VAE model at epoch 8\n",
      "Epoch 9, Training Loss: 147.88727329508464, Validation Loss: 147.8560028889974\n",
      "Saving Standard_VAE model at epoch 9\n",
      "Epoch 10, Training Loss: 146.97939158121744, Validation Loss: 147.12885144042968\n",
      "Saving Standard_VAE model at epoch 10\n",
      "Best Standard_VAE model saved as best_Standard_VAE_epoch_9.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff488a19f77544f2897a2f6bf7fb3b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines+markers',\n",
       "              'name': 'Training Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'afdb25bf-818f-48d3-8a0e-e54da4e43351',\n",
       "              'x': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       "              'y': [226.89376039632162, 184.66886783854167, 164.75998073323566,\n",
       "                    156.47857543945312, 153.54923905436198, 151.72211771647136,\n",
       "                    150.495251180013, 149.39842336018881, 147.88727329508464,\n",
       "                    146.97939158121744]},\n",
       "             {'mode': 'lines+markers',\n",
       "              'name': 'Validation Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'da655403-db7c-4118-83f3-503c0dc824c9',\n",
       "              'x': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       "              'y': [193.1483650716146, 174.04674861653646, 159.01668273925782,\n",
       "                    154.85309574381512, 152.9229608561198, 151.54623213704426,\n",
       "                    150.29708162434895, 149.78097778320313, 147.8560028889974,\n",
       "                    147.12885144042968]}],\n",
       "    'layout': {'height': 600,\n",
       "               'template': '...',\n",
       "               'title': {'text': 'Interactive Standard_VAE Training and Validation Loss'},\n",
       "               'width': 800,\n",
       "               'xaxis': {'title': {'text': 'Epoch'}},\n",
       "               'yaxis': {'title': {'text': 'Loss'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run cell to train Standard VAE\n",
    "print(f'Training parameters: batch_size={batch_size}, learning_rate={learning_rate}, epochs={epochs}')\n",
    "print(\"Training Standard_VAE...\")\n",
    "Optimiser_Standard = torch.optim.Adam(Standard_VAE.parameters(), lr=learning_rate)\n",
    "Trained_Standard_VAE = train_vae(Standard_VAE, trainloader, validationloader, loss_function_standard, Optimiser_Standard, epochs, DEVICE, \"Standard_VAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameters: batch_size=64, learning_rate=0.001, epochs=10\n",
      "Training Lake_VAE...\n",
      "Epoch 1, Training Loss: 196.97447145589192, Validation Loss: 157.30794738769532\n",
      "Saving Lake_VAE model at epoch 1\n",
      "Epoch 2, Training Loss: 146.9710359802246, Validation Loss: 139.83250321451823\n",
      "Saving Lake_VAE model at epoch 2\n",
      "Epoch 3, Training Loss: 135.70760551961263, Validation Loss: 133.41635420735676\n",
      "Saving Lake_VAE model at epoch 3\n",
      "Epoch 4, Training Loss: 131.69676136271158, Validation Loss: 130.97847662353516\n",
      "Saving Lake_VAE model at epoch 4\n",
      "Epoch 5, Training Loss: 129.85162173461913, Validation Loss: 129.6448965250651\n",
      "Saving Lake_VAE model at epoch 5\n",
      "Epoch 6, Training Loss: 128.53322006225585, Validation Loss: 128.36548286946615\n",
      "Saving Lake_VAE model at epoch 6\n",
      "Epoch 7, Training Loss: 127.50781346638998, Validation Loss: 128.03276088460285\n",
      "Saving Lake_VAE model at epoch 7\n",
      "Epoch 8, Training Loss: 126.6456785176595, Validation Loss: 126.97166536458333\n",
      "Saving Lake_VAE model at epoch 8\n",
      "Epoch 9, Training Loss: 125.97908441162109, Validation Loss: 126.47102274576822\n",
      "Saving Lake_VAE model at epoch 9\n",
      "Epoch 10, Training Loss: 125.418255859375, Validation Loss: 125.92981844075521\n",
      "Saving Lake_VAE model at epoch 10\n",
      "Best Lake_VAE model saved as best_Lake_VAE_epoch_9.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0386fc0db247c584c28e0587be9da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines+markers',\n",
       "              'name': 'Training Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': '8651cf43-7c5a-4bf3-8f33-aa9be1d3ca0c',\n",
       "              'x': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       "              'y': [196.97447145589192, 146.9710359802246, 135.70760551961263,\n",
       "                    131.69676136271158, 129.85162173461913, 128.53322006225585,\n",
       "                    127.50781346638998, 126.6456785176595, 125.97908441162109,\n",
       "                    125.418255859375]},\n",
       "             {'mode': 'lines+markers',\n",
       "              'name': 'Validation Loss',\n",
       "              'type': 'scatter',\n",
       "              'uid': '98df721b-b72b-4c13-9c03-17e1744d826e',\n",
       "              'x': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       "              'y': [157.30794738769532, 139.83250321451823, 133.41635420735676,\n",
       "                    130.97847662353516, 129.6448965250651, 128.36548286946615,\n",
       "                    128.03276088460285, 126.97166536458333, 126.47102274576822,\n",
       "                    125.92981844075521]}],\n",
       "    'layout': {'height': 600,\n",
       "               'template': '...',\n",
       "               'title': {'text': 'Interactive Lake_VAE Training and Validation Loss'},\n",
       "               'width': 800,\n",
       "               'xaxis': {'title': {'text': 'Epoch'}},\n",
       "               'yaxis': {'title': {'text': 'Loss'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run cell to train Lake VAE\n",
    "print(f'Training parameters: batch_size={batch_size}, learning_rate={learning_rate}, epochs={epochs}')\n",
    "print(\"Training Lake_VAE...\")\n",
    "Optimiser_Lake = torch.optim.Adam(Lake_VAE.parameters(), lr=learning_rate)\n",
    "Trained_Lake_VAE = train_vae(Lake_VAE, trainloader, validationloader, loss_function_lake, Optimiser_Lake, epochs, DEVICE, \"Lake_VAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training only uses 25MiB of GPU memory. Perhaps could try Jupyterlab and jupyterlab-nvdashboard to monitor GPU usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_standard_vae_path = 'best_Standard_VAE_epoch_X.pth'\n",
    "# best_lake_vae_path = 'best_Lake_VAE_epoch_Y.pth'\n",
    "\n",
    "standard_vae_path = 'Standard_VAE_9.pth'\n",
    "lake_vae_path = 'Lake_VAE_9.pth'\n",
    "\n",
    "# Load the best models\n",
    "Standard_VAE = StandardVAE().to(DEVICE)\n",
    "load_model(Standard_VAE, standard_vae_path, DEVICE)\n",
    "\n",
    "Lake_VAE = LakeVAE().to(DEVICE)\n",
    "load_model(Lake_VAE, lake_vae_path, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising Trained Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_processing = False\n",
    "\n",
    "def reconstruct_from_latent_space(model, latent_point, pca, device):\n",
    "    original_latent = pca.inverse_transform([latent_point])\n",
    "    original_latent_tensor = torch.from_numpy(original_latent).float().to(device)\n",
    "    reconstructed_img = model.decode(original_latent_tensor).cpu()\n",
    "    return reconstructed_img[0].squeeze()\n",
    "\n",
    "def plot_latent_space_interactive(model, data_loader, device, vaename, dataname, num_samples=1000):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, label in data_loader:\n",
    "            data = data.to(device)\n",
    "            mu, _ = model.encode(data)\n",
    "            latents.append(mu.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "            if len(latents) * data_loader.batch_size > num_samples:\n",
    "                break\n",
    "\n",
    "    latents = np.concatenate(latents, axis=0)[:num_samples]\n",
    "    labels = np.concatenate(labels, axis=0)[:num_samples]\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    latents_reduced = pca.fit_transform(latents)\n",
    "\n",
    "    # Define a custom color scale (10 different colors for digits 0-9)\n",
    "    custom_color_scale = ['#636EFA', '#EF553B', '#00CC96', '#AB63FA', '#FFA15A',\n",
    "                          '#19D3F3', '#FF6692', '#B6E880', '#FF97FF', '#FECB52']\n",
    "\n",
    "    # Create traces for each digit with hover text\n",
    "    traces = []\n",
    "    for digit in range(10):\n",
    "        digit_indices = np.where(labels == digit)[0]\n",
    "        trace = go.Scatter(\n",
    "            x=latents_reduced[digit_indices, 0], y=latents_reduced[digit_indices, 1],\n",
    "            mode='markers', marker=dict(color=custom_color_scale[digit], size=10),\n",
    "            name=str(digit),\n",
    "            hoverinfo='text',\n",
    "            text=[f'Label: {digit}, Pos: ({x:.2f}, {y:.2f})' for x, y in latents_reduced[digit_indices]]\n",
    "        )\n",
    "        traces.append(trace)\n",
    "\n",
    "    # Plotly figure with separate traces\n",
    "    fig = go.FigureWidget(traces)\n",
    "    fig.update_layout(\n",
    "        title=f'{vaename} Latent Space Visualisation of MNIST {dataname} Data',\n",
    "        xaxis_title='Principal Component 1',\n",
    "        yaxis_title='Principal Component 2',\n",
    "        width=800, height=600,\n",
    "        legend_title_text='Digit Label'\n",
    "    )\n",
    "\n",
    "    def update_image(trace, points, selector):\n",
    "        global is_processing\n",
    "        if is_processing:\n",
    "            return\n",
    "    \n",
    "        is_processing = True\n",
    "    \n",
    "        if points.point_inds:\n",
    "            idx = points.point_inds[0]\n",
    "            latent_point = latents_reduced[idx]\n",
    "            img = reconstruct_from_latent_space(model, latent_point, pca, device)\n",
    "            plt.imshow(img.detach().numpy(), cmap='gray')\n",
    "            plt.axis('off')\n",
    "            clear_output(wait=True)\n",
    "            display(plt.gcf())\n",
    "    \n",
    "        is_processing = False\n",
    "\n",
    "    for trace in fig.data:\n",
    "        trace.on_click(update_image)\n",
    "\n",
    "    display(fig)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354544c1d9844bb68e7e9230ed1f04cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'hoverinfo': 'text',\n",
       "              'marker': {'color': '#636EFA', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '0',\n",
       "              'text': [Label: 0, Pos: (2.49, -0.01), Label: 0, Pos: (2.81, 1.56),\n",
       "                       Label: 0, Pos: (2.69, 0.50), ..., Label: 0, Pos: (2.68,\n",
       "                       0.98), Label: 0, Pos: (2.73, -0.38), Label: 0, Pos: (2.58,\n",
       "                       -0.35)],\n",
       "              'type': 'scatter',\n",
       "              'uid': '581d373a-bd57-4a76-9c8f-6ecd65a4f751',\n",
       "              'x': array([2.4901571, 2.8095226, 2.686181 , ..., 2.6832032, 2.7348704, 2.582164 ],\n",
       "                         dtype=float32),\n",
       "              'y': array([-0.01425579,  1.5630798 ,  0.49716645, ...,  0.9761483 , -0.38007715,\n",
       "                          -0.34660178], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#EF553B', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '1',\n",
       "              'text': [Label: 1, Pos: (-0.88, -1.61), Label: 1, Pos: (-1.46,\n",
       "                       -1.59), Label: 1, Pos: (-1.87, -1.37), ..., Label: 1, Pos:\n",
       "                       (-1.52, -1.59), Label: 1, Pos: (-1.74, -1.38), Label: 1,\n",
       "                       Pos: (-1.24, -1.45)],\n",
       "              'type': 'scatter',\n",
       "              'uid': '75232016-a04d-4a94-9a78-3f3c0a390501',\n",
       "              'x': array([-0.88047343, -1.4589716 , -1.865721  , ..., -1.5205076 , -1.7356701 ,\n",
       "                          -1.2365584 ], dtype=float32),\n",
       "              'y': array([-1.6149985, -1.5918033, -1.374515 , ..., -1.5860524, -1.3795806,\n",
       "                          -1.4473419], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#00CC96', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '2',\n",
       "              'text': [Label: 2, Pos: (1.09, -0.39), Label: 2, Pos: (0.27, -0.60),\n",
       "                       Label: 2, Pos: (1.27, -0.31), ..., Label: 2, Pos: (0.82,\n",
       "                       -0.20), Label: 2, Pos: (0.08, 0.19), Label: 2, Pos: (0.63,\n",
       "                       0.00)],\n",
       "              'type': 'scatter',\n",
       "              'uid': '12f0971b-0391-4cb1-89ea-880fcbb1bb74',\n",
       "              'x': array([1.0940988 , 0.2658171 , 1.2739717 , ..., 0.8210055 , 0.07931584,\n",
       "                          0.62823725], dtype=float32),\n",
       "              'y': array([-0.38634366, -0.60379255, -0.31179014, ..., -0.20474593,  0.19008113,\n",
       "                           0.00248914], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#AB63FA', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '3',\n",
       "              'text': [Label: 3, Pos: (0.26, -0.10), Label: 3, Pos: (0.19, -0.36),\n",
       "                       Label: 3, Pos: (0.64, 0.70), ..., Label: 3, Pos: (-0.07,\n",
       "                       -0.23), Label: 3, Pos: (-0.75, 0.58), Label: 3, Pos: (0.13,\n",
       "                       0.22)],\n",
       "              'type': 'scatter',\n",
       "              'uid': 'd004a1c1-8957-419e-9212-dcbdf5436540',\n",
       "              'x': array([ 0.26452342,  0.19166243,  0.63624483, ..., -0.07235811, -0.74533665,\n",
       "                           0.13359055], dtype=float32),\n",
       "              'y': array([-0.10476358, -0.3603822 ,  0.69916004, ..., -0.23052286,  0.5785665 ,\n",
       "                           0.22212538], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#FFA15A', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '4',\n",
       "              'text': [Label: 4, Pos: (0.02, 1.38), Label: 4, Pos: (-1.04, 0.38),\n",
       "                       Label: 4, Pos: (0.07, 1.26), ..., Label: 4, Pos: (-0.58,\n",
       "                       0.55), Label: 4, Pos: (-1.44, -0.28), Label: 4, Pos: (-1.21,\n",
       "                       0.61)],\n",
       "              'type': 'scatter',\n",
       "              'uid': '2f2d5da4-f991-476d-a08a-29672d7313d6',\n",
       "              'x': array([ 0.0165988 , -1.041991  ,  0.06698444, ..., -0.5767585 , -1.443854  ,\n",
       "                          -1.2064228 ], dtype=float32),\n",
       "              'y': array([ 1.3806688 ,  0.38296935,  1.2644631 , ...,  0.5450616 , -0.28191903,\n",
       "                           0.6065515 ], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#19D3F3', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '5',\n",
       "              'text': [Label: 5, Pos: (-0.84, -0.18), Label: 5, Pos: (0.54,\n",
       "                       -0.19), Label: 5, Pos: (-0.25, 0.90), ..., Label: 5, Pos:\n",
       "                       (0.29, -0.33), Label: 5, Pos: (0.43, 1.13), Label: 5, Pos:\n",
       "                       (0.06, -0.80)],\n",
       "              'type': 'scatter',\n",
       "              'uid': '79209999-c150-490e-aa4b-d147822b9072',\n",
       "              'x': array([-0.83919877,  0.54101855, -0.24675016, ...,  0.28702843,  0.42583233,\n",
       "                           0.06127275], dtype=float32),\n",
       "              'y': array([-0.1813664 , -0.19136311,  0.8968074 , ..., -0.328867  ,  1.1341428 ,\n",
       "                          -0.8029639 ], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#FF6692', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '6',\n",
       "              'text': [Label: 6, Pos: (2.62, -1.31), Label: 6, Pos: (0.25, -1.79),\n",
       "                       Label: 6, Pos: (1.02, -2.09), ..., Label: 6, Pos: (1.95,\n",
       "                       -2.11), Label: 6, Pos: (1.47, -1.31), Label: 6, Pos: (0.79,\n",
       "                       2.22)],\n",
       "              'type': 'scatter',\n",
       "              'uid': '6ce88414-42da-4309-95a2-9619649b5bb8',\n",
       "              'x': array([2.6165245 , 0.24876556, 1.0177583 , ..., 1.9502604 , 1.4652507 ,\n",
       "                          0.79073817], dtype=float32),\n",
       "              'y': array([-1.3149678, -1.7921966, -2.0943663, ..., -2.110968 , -1.3092744,\n",
       "                           2.220118 ], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#B6E880', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '7',\n",
       "              'text': [Label: 7, Pos: (-2.58, 1.29), Label: 7, Pos: (-0.53,\n",
       "                       -0.24), Label: 7, Pos: (0.48, 2.81), ..., Label: 7, Pos:\n",
       "                       (-2.73, 0.94), Label: 7, Pos: (-1.22, 2.85), Label: 7, Pos:\n",
       "                       (-1.02, -0.43)],\n",
       "              'type': 'scatter',\n",
       "              'uid': '1af81ccc-d224-4c3e-a5c7-94194f4f75bc',\n",
       "              'x': array([-2.5844016 , -0.53313196,  0.48207325, ..., -2.726876  , -1.2161441 ,\n",
       "                          -1.0154165 ], dtype=float32),\n",
       "              'y': array([ 1.2942967 , -0.23694715,  2.8075795 , ...,  0.944644  ,  2.85264   ,\n",
       "                          -0.42544076], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#FF97FF', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '8',\n",
       "              'text': [Label: 8, Pos: (-0.13, -0.53), Label: 8, Pos: (-0.10,\n",
       "                       -0.30), Label: 8, Pos: (-0.22, -0.43), ..., Label: 8, Pos:\n",
       "                       (-0.00, 0.44), Label: 8, Pos: (-0.43, -0.25), Label: 8, Pos:\n",
       "                       (0.38, 0.40)],\n",
       "              'type': 'scatter',\n",
       "              'uid': '261cf7f7-56c0-4ec0-934f-dd63a2242551',\n",
       "              'x': array([-0.13466254, -0.10488938, -0.21711028, ..., -0.00075753, -0.427788  ,\n",
       "                           0.38480443], dtype=float32),\n",
       "              'y': array([-0.5285753 , -0.30107188, -0.4289059 , ...,  0.43645993, -0.24957344,\n",
       "                           0.40354842], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#FECB52', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '9',\n",
       "              'text': [Label: 9, Pos: (-0.79, 0.75), Label: 9, Pos: (-1.09, 0.94),\n",
       "                       Label: 9, Pos: (-1.67, -0.41), ..., Label: 9, Pos: (-1.80,\n",
       "                       0.13), Label: 9, Pos: (-0.77, 1.41), Label: 9, Pos: (-1.52,\n",
       "                       0.27)],\n",
       "              'type': 'scatter',\n",
       "              'uid': 'da14ce5e-6aa8-4275-b7b4-eacc43deed8f',\n",
       "              'x': array([-0.78928936, -1.0923663 , -1.6666253 , ..., -1.7991354 , -0.76987004,\n",
       "                          -1.522738  ], dtype=float32),\n",
       "              'y': array([ 0.75169563,  0.93668276, -0.4114905 , ...,  0.12777634,  1.406373  ,\n",
       "                           0.27005526], dtype=float32)}],\n",
       "    'layout': {'height': 600,\n",
       "               'legend': {'title': {'text': 'Digit Label'}},\n",
       "               'template': '...',\n",
       "               'title': {'text': 'Standard VAE Epochs Latent Space Visualisation of MNIST Validation Data'},\n",
       "               'width': 800,\n",
       "               'xaxis': {'title': {'text': 'Principal Component 1'}},\n",
       "               'yaxis': {'title': {'text': 'Principal Component 2'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_latent_space_interactive(Standard_VAE, trainloader, DEVICE, 'Standard VAE Epochs' ,'Training', num_samples=500)\n",
    "plot_latent_space_interactive(Standard_VAE, validationloader, DEVICE, 'Standard VAE Epochs' ,'Validation', num_samples=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baaf182147cf478b90fcc9f4f4f7bc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'hoverinfo': 'text',\n",
       "              'marker': {'color': '#636EFA', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '0',\n",
       "              'text': [Label: 0, Pos: (2.38, -0.38), Label: 0, Pos: (1.01, -0.05),\n",
       "                       Label: 0, Pos: (0.65, -0.70), ..., Label: 0, Pos: (0.53,\n",
       "                       -0.18), Label: 0, Pos: (0.11, -1.20), Label: 0, Pos: (1.58,\n",
       "                       -1.55)],\n",
       "              'type': 'scatter',\n",
       "              'uid': '6218fe83-b214-4540-9664-aada615ec135',\n",
       "              'x': array([2.378218  , 1.0071332 , 0.6456206 , ..., 0.5291387 , 0.11081575,\n",
       "                          1.5753206 ], dtype=float32),\n",
       "              'y': array([-0.38112515, -0.05147202, -0.7019567 , ..., -0.17912616, -1.2015626 ,\n",
       "                          -1.553151  ], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#EF553B', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '1',\n",
       "              'text': [Label: 1, Pos: (-0.13, -0.58), Label: 1, Pos: (-0.44,\n",
       "                       -0.45), Label: 1, Pos: (-0.88, -0.03), ..., Label: 1, Pos:\n",
       "                       (-0.91, 0.55), Label: 1, Pos: (-1.11, 0.62), Label: 1, Pos:\n",
       "                       (-0.73, 0.53)],\n",
       "              'type': 'scatter',\n",
       "              'uid': 'ad36b707-903b-4162-9098-30ef7f7cafee',\n",
       "              'x': array([-0.13450879, -0.44204944, -0.87889135, ..., -0.9099079 , -1.1136279 ,\n",
       "                          -0.734537  ], dtype=float32),\n",
       "              'y': array([-0.5762512 , -0.45400771, -0.03228408, ...,  0.5498978 ,  0.62131596,\n",
       "                           0.53214   ], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#00CC96', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '2',\n",
       "              'text': [Label: 2, Pos: (0.09, -0.04), Label: 2, Pos: (0.97, -0.50),\n",
       "                       Label: 2, Pos: (1.64, -1.32), ..., Label: 2, Pos: (0.58,\n",
       "                       -0.87), Label: 2, Pos: (0.58, -0.20), Label: 2, Pos: (-1.04,\n",
       "                       -0.60)],\n",
       "              'type': 'scatter',\n",
       "              'uid': '42cc6a10-4a7d-468f-afd8-e662130c1911',\n",
       "              'x': array([ 0.093971  ,  0.97476786,  1.6429268 , ...,  0.57741326,  0.5849412 ,\n",
       "                          -1.0434414 ], dtype=float32),\n",
       "              'y': array([-0.03751137, -0.50370055, -1.3192976 , ..., -0.87292635, -0.19775611,\n",
       "                          -0.5976021 ], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#AB63FA', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '3',\n",
       "              'text': [Label: 3, Pos: (-1.19, -0.10), Label: 3, Pos: (-0.60,\n",
       "                       -0.74), Label: 3, Pos: (-0.24, -0.58), ..., Label: 3, Pos:\n",
       "                       (-0.25, -1.08), Label: 3, Pos: (-0.70, -0.79), Label: 3,\n",
       "                       Pos: (-0.57, -0.65)],\n",
       "              'type': 'scatter',\n",
       "              'uid': '5a729f05-2167-4100-82a0-003c6fe82a77',\n",
       "              'x': array([-1.192556  , -0.59607774, -0.24185187, ..., -0.2504744 , -0.6981157 ,\n",
       "                          -0.57087874], dtype=float32),\n",
       "              'y': array([-0.10369004, -0.74257785, -0.5790606 , ..., -1.0757313 , -0.7896143 ,\n",
       "                          -0.6530072 ], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#FFA15A', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '4',\n",
       "              'text': [Label: 4, Pos: (1.28, 1.18), Label: 4, Pos: (0.48, 0.49),\n",
       "                       Label: 4, Pos: (-0.03, 0.95), ..., Label: 4, Pos: (0.11,\n",
       "                       0.04), Label: 4, Pos: (0.34, 0.58), Label: 4, Pos: (0.89,\n",
       "                       0.09)],\n",
       "              'type': 'scatter',\n",
       "              'uid': 'e5d44eb0-9699-44b4-abbb-7956d07c79c2',\n",
       "              'x': array([ 1.2796332 ,  0.48210776, -0.03424145, ...,  0.10768943,  0.33597472,\n",
       "                           0.8897542 ], dtype=float32),\n",
       "              'y': array([1.177416  , 0.4929539 , 0.94638515, ..., 0.04329674, 0.5764901 ,\n",
       "                          0.0890924 ], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#19D3F3', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '5',\n",
       "              'text': [Label: 5, Pos: (-0.68, -0.66), Label: 5, Pos: (1.07,\n",
       "                       -0.32), Label: 5, Pos: (-1.07, -0.43), ..., Label: 5, Pos:\n",
       "                       (-1.64, -0.25), Label: 5, Pos: (-1.15, -0.26), Label: 5,\n",
       "                       Pos: (0.44, -0.26)],\n",
       "              'type': 'scatter',\n",
       "              'uid': '36ab18ea-e87d-4b0f-9f75-16c61911bf4a',\n",
       "              'x': array([-0.6778756 ,  1.0743729 , -1.0726622 , ..., -1.6449177 , -1.150642  ,\n",
       "                           0.44464347], dtype=float32),\n",
       "              'y': array([-0.6555617 , -0.3208694 , -0.43338245, ..., -0.24591352, -0.2590809 ,\n",
       "                          -0.26157856], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#FF6692', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '6',\n",
       "              'text': [Label: 6, Pos: (0.48, -0.21), Label: 6, Pos: (-0.76,\n",
       "                       -0.65), Label: 6, Pos: (1.01, -0.99), ..., Label: 6, Pos:\n",
       "                       (-0.05, -1.04), Label: 6, Pos: (2.15, -0.20), Label: 6, Pos:\n",
       "                       (1.06, 0.09)],\n",
       "              'type': 'scatter',\n",
       "              'uid': '18c783a1-9763-42de-8b2a-a64f806ef96e',\n",
       "              'x': array([ 0.47979492, -0.76195264,  1.0130209 , ..., -0.05021368,  2.1482484 ,\n",
       "                           1.0631076 ], dtype=float32),\n",
       "              'y': array([-0.20897992, -0.650421  , -0.98887986, ..., -1.0381716 , -0.19533479,\n",
       "                           0.09099109], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#B6E880', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '7',\n",
       "              'text': [Label: 7, Pos: (1.83, 2.54), Label: 7, Pos: (0.34, 0.99),\n",
       "                       Label: 7, Pos: (1.05, 2.36), ..., Label: 7, Pos: (-0.27,\n",
       "                       1.16), Label: 7, Pos: (0.95, 1.74), Label: 7, Pos: (1.17,\n",
       "                       0.91)],\n",
       "              'type': 'scatter',\n",
       "              'uid': 'ec6d7b40-b473-4ca9-8bfa-824f701f0d11',\n",
       "              'x': array([ 1.8316513 ,  0.34387898,  1.051437  , ..., -0.27332938,  0.949916  ,\n",
       "                           1.1730431 ], dtype=float32),\n",
       "              'y': array([2.535142 , 0.9914095, 2.36385  , ..., 1.1615291, 1.7411098, 0.9106492],\n",
       "                         dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#FF97FF', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '8',\n",
       "              'text': [Label: 8, Pos: (-1.19, -0.20), Label: 8, Pos: (0.48,\n",
       "                       -0.48), Label: 8, Pos: (-1.22, 0.31), ..., Label: 8, Pos:\n",
       "                       (-0.91, -0.49), Label: 8, Pos: (-1.04, -0.29), Label: 8,\n",
       "                       Pos: (-1.60, -0.24)],\n",
       "              'type': 'scatter',\n",
       "              'uid': 'a5952a45-35ea-495a-a009-a57e1c9f357d',\n",
       "              'x': array([-1.1934193 ,  0.47974005, -1.2164223 , ..., -0.9101927 , -1.0364366 ,\n",
       "                          -1.599134  ], dtype=float32),\n",
       "              'y': array([-0.19879717, -0.48453677,  0.30583376, ..., -0.48564616, -0.28717816,\n",
       "                          -0.23730025], dtype=float32)},\n",
       "             {'hoverinfo': 'text',\n",
       "              'marker': {'color': '#FECB52', 'size': 10},\n",
       "              'mode': 'markers',\n",
       "              'name': '9',\n",
       "              'text': [Label: 9, Pos: (-0.67, 0.87), Label: 9, Pos: (-0.95,\n",
       "                       -0.04), Label: 9, Pos: (-0.74, 0.75), ..., Label: 9, Pos:\n",
       "                       (-0.65, 0.83), Label: 9, Pos: (0.95, 1.06), Label: 9, Pos:\n",
       "                       (-0.90, 1.12)],\n",
       "              'type': 'scatter',\n",
       "              'uid': '0ccea7bc-3892-4ebb-afc9-ce29b2bc0e9b',\n",
       "              'x': array([-0.66833705, -0.9491053 , -0.7405709 , ..., -0.64859957,  0.9462968 ,\n",
       "                          -0.898697  ], dtype=float32),\n",
       "              'y': array([ 0.8664493 , -0.03635283,  0.75314164, ...,  0.82518625,  1.0594265 ,\n",
       "                           1.1157228 ], dtype=float32)}],\n",
       "    'layout': {'height': 600,\n",
       "               'legend': {'title': {'text': 'Digit Label'}},\n",
       "               'template': '...',\n",
       "               'title': {'text': 'layer-constrained VAE Epochs Latent Space Visualisation of MNIST Validiation Data'},\n",
       "               'width': 800,\n",
       "               'xaxis': {'title': {'text': 'Principal Component 1'}},\n",
       "               'yaxis': {'title': {'text': 'Principal Component 2'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_latent_space_interactive(Lake_VAE, trainloader, DEVICE, 'layer-constrained VAE Epochs', 'Training', num_samples=500)\n",
    "plot_latent_space_interactive(Lake_VAE, validationloader, DEVICE, 'layer-constrained VAE Epochs', 'Validiation', num_samples=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Density Estimation Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 900 MNIST validation images (labelled 0) and 100 GAN generated MNIST images (labelled 1) to create a combined testing dataset. \n",
    "Then attempting to see if the LAKE anomaly detection can find the synthetic images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of pixel values: 0.0 to 1.0\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the validation dataset\n",
    "mnist_validation_data = validation_dataset \n",
    "\n",
    "# Extract MNIST images and labels\n",
    "mnist_images = mnist_validation_data.dataset.data[mnist_validation_data.indices][:900]\n",
    "mnist_labels = torch.zeros(900)  # Label '0' for real MNIST data\n",
    "\n",
    "# Load GAN generated images\n",
    "gan_images_path = 'gan_generated_mnist_images.pt'\n",
    "gan_images = torch.load(gan_images_path)\n",
    "\n",
    "# Normalize the MNIST and GAN images if they are not already\n",
    "mnist_images = mnist_images.float() / 255.0\n",
    "mnist_images = mnist_images[:,None]\n",
    "gan_images = gan_images.float() / 255.0 if gan_images.max() > 1.0 else gan_images\n",
    "\n",
    "# Add a pure white image as an anomaly\n",
    "white_image = torch.ones(1, 1, 28, 28)\n",
    "white_label = torch.tensor([1])  # Anomaly label\n",
    "\n",
    "# Combine MNIST, GAN, and white image into one dataset\n",
    "combined_images = torch.cat((mnist_images, gan_images, white_image), dim=0)\n",
    "combined_labels = torch.cat((mnist_labels, torch.ones(100), white_label), dim=0)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "combined_dataset = TensorDataset(combined_images, combined_labels)\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Check the normalization\n",
    "max_pixel_value = combined_images.max()\n",
    "min_pixel_value = combined_images.min()\n",
    "print(f\"Range of pixel values: {min_pixel_value} to {max_pixel_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 12)\n",
      "(1001, 12)\n"
     ]
    }
   ],
   "source": [
    "# Function to encode dataset and compute reconstruction errors\n",
    "def encode_and_reconstruct(model, dataloader, device):\n",
    "    model.eval()\n",
    "    ws = []\n",
    "    rec_errors = []\n",
    "    with torch.no_grad():\n",
    "        for x, _ in dataloader:\n",
    "            x = x.to(device)\n",
    "            # Get w and x' in forward pass\n",
    "            recon_x, _, _, _, _, _, w, _, _, _ = model(x)\n",
    "\n",
    "            # Store w\n",
    "            ws.append(w.cpu()) \n",
    "\n",
    "            # Calc and store rec_eu and rec_co\n",
    "            rec_euclidean = torch.norm(x - recon_x, p=2, dim=(1, 2, 3))\n",
    "            rec_cosine = F.cosine_similarity(x.view(x.size(0), -1), recon_x.view(recon_x.size(0), -1), dim=1)\n",
    "            r = torch.stack((rec_euclidean, rec_cosine), dim=1)\n",
    "            rec_errors.append(r.cpu()) \n",
    "            \n",
    "    return torch.cat(ws, dim=0), torch.cat(rec_errors, dim=0)\n",
    "    \n",
    "Lake_VAE.to(DEVICE)\n",
    "\n",
    "# For Training Data\n",
    "encoded_ws, reconstruction_rs = encode_and_reconstruct(Lake_VAE, trainloader, DEVICE)\n",
    "assert not torch.isnan(encoded_ws).any(), \"NaNs in encoded_ws\"\n",
    "assert not torch.isnan(reconstruction_rs).any(), \"NaNs in reconstruction_rs\"\n",
    "C_Train = np.hstack((encoded_ws, reconstruction_rs))\n",
    "print(C_Train.shape)\n",
    "\n",
    "# Testing Data\n",
    "encoded_ws, reconstruction_rs = encode_and_reconstruct(Lake_VAE, combined_loader, DEVICE)\n",
    "assert not torch.isnan(encoded_ws).any(), \"NaNs in encoded_ws\"\n",
    "assert not torch.isnan(reconstruction_rs).any(), \"NaNs in reconstruction_rs\"\n",
    "C = np.hstack((encoded_ws, reconstruction_rs))\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main things that affect the anomaly detection results. <br>\n",
    "A. The model used (how well the data is compressed). The ability of your VAE to compress and reconstruct data is critical. It is contained within w and r. <br>\n",
    "B. The KDE's bandwidth setting, corresponding to the smoothness of the density estimate. <br>\n",
    "If it's too narrow, you might have a very bumpy estimate that's sensitive to noise. <br>\n",
    "If it's too wide, the estimate might be too smooth and anomalies could be missed because they blend in with the normal data. <br>\n",
    "C. Anomaly detection threshold. What proportion of your dataset you set expected to be an anomaly. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[214], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mexp(log_density)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Perform KDE\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m kde_model \u001b[38;5;241m=\u001b[39m \u001b[43mperform_kde\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC_Train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m density_estimates \u001b[38;5;241m=\u001b[39m estimate_density(kde_model, C)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Determine anomaly threshold and detect anomalies\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[214], line 7\u001b[0m, in \u001b[0;36mperform_kde\u001b[0;34m(C_Train)\u001b[0m\n\u001b[1;32m      5\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbandwidth\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m100\u001b[39m)}\n\u001b[1;32m      6\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(KernelDensity(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgaussian\u001b[39m\u001b[38;5;124m'\u001b[39m), params, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC_Train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal bandwidth: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mbandwidth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m kde \u001b[38;5;241m=\u001b[39m KernelDensity(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgaussian\u001b[39m\u001b[38;5;124m'\u001b[39m, bandwidth\u001b[38;5;241m=\u001b[39mgrid\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mbandwidth)\n",
      "File \u001b[0;32m~/Documents/LAKE/.venv/lib64/python3.9/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LAKE/.venv/lib64/python3.9/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/LAKE/.venv/lib64/python3.9/site-packages/sklearn/model_selection/_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LAKE/.venv/lib64/python3.9/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/LAKE/.venv/lib64/python3.9/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LAKE/.venv/lib64/python3.9/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Documents/LAKE/.venv/lib64/python3.9/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Documents/LAKE/.venv/lib64/python3.9/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LAKE/.venv/lib64/python3.9/site-packages/sklearn/model_selection/_validation.py:751\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    748\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    750\u001b[0m fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m--> 751\u001b[0m test_scores \u001b[38;5;241m=\u001b[39m \u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m score_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m-\u001b[39m fit_time\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_train_score:\n",
      "File \u001b[0;32m~/Documents/LAKE/.venv/lib64/python3.9/site-packages/sklearn/model_selection/_validation.py:808\u001b[0m, in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 808\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    810\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scorer(estimator, X_test, y_test)\n",
      "File \u001b[0;32m~/Documents/LAKE/.venv/lib64/python3.9/site-packages/sklearn/metrics/_scorer.py:527\u001b[0m, in \u001b[0;36m_PassthroughScorer.__call__\u001b[0;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Method that wraps estimator.score\"\"\"\u001b[39;00m\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LAKE/.venv/lib64/python3.9/site-packages/sklearn/neighbors/_kde.py:303\u001b[0m, in \u001b[0;36mKernelDensity.score\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    284\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the total log-likelihood under the model.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m        data.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/LAKE/.venv/lib64/python3.9/site-packages/sklearn/neighbors/_kde.py:271\u001b[0m, in \u001b[0;36mKernelDensity.score_samples\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    269\u001b[0m     N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39msum_weight\n\u001b[1;32m    270\u001b[0m atol_N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matol \u001b[38;5;241m*\u001b[39m N\n\u001b[0;32m--> 271\u001b[0m log_density \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_density\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbandwidth_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matol_N\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbreadth_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbreadth_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m log_density \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(N)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_density\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# KernelDensity from sklearn represents fh(s) = (1/n) ∑[i=1 to n] Kh(s - ci)\n",
    "from joblib import dump\n",
    "\n",
    "def perform_kde(C_Train):\n",
    "    # Doing k-fold validation on to find good bandwidth value\n",
    "    params = {'bandwidth': np.linspace(0.01, 0.2, 20)}\n",
    "    grid = GridSearchCV(KernelDensity(kernel='gaussian'), params, cv=5)\n",
    "    grid.fit(C_Train)\n",
    "    print(f\"Optimal bandwidth: {grid.best_estimator_.bandwidth}\")\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=grid.best_estimator_.bandwidth)\n",
    "    \n",
    "    #kde = KernelDensity(kernel='gaussian', bandwidth=0.05) # Manual bandwidth setting\n",
    "    kde.fit(C_Train)\n",
    "    return kde\n",
    "\n",
    "# Function to estimate density from C values\n",
    "def estimate_density(kde, C):\n",
    "    log_density = kde.score_samples(C)\n",
    "    return np.exp(log_density)\n",
    "\n",
    "# Calibrate and Save KDE\n",
    "kde_model = perform_kde(C_Train)\n",
    "dump(kde_model, 'kde_model.joblib')\n",
    "\n",
    "density_estimates = estimate_density(kde_model, C)\n",
    "\n",
    "# Determine anomaly threshold and detect anomalies\n",
    "threshold = np.percentile(density_estimates, 10)\n",
    "anomalies = density_estimates < threshold\n",
    "\n",
    "# Calculate and print anomaly detection results\n",
    "detected_anomalies = np.sum(anomalies[-101:]) \n",
    "print(f\"Detected {detected_anomalies} anomalies out of 101 synthetic images.\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(density_estimates, bins=100, alpha=0.5, color='blue', label='Density Scores')\n",
    "plt.axvline(threshold, color='red', linestyle='--', label='Threshold')\n",
    "plt.legend()\n",
    "plt.title('Density Estimates and Anomaly Threshold')\n",
    "plt.xlabel('Density Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "kde_model = load('kde_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
